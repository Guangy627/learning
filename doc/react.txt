问题-- llm在reasoning和acting上各有进展但是2023年以前两者都是分开的，reasoning cot 让模型生成一大串思维步骤，但是这些推理是封闭的，完全依赖的是模型内部的知识结构
容易油hallucination； acting model 基于语言的计划与execute比如说控制agnet在环境中的routing，但是这种方法本身会缺乏系统性的推理容易陷入错误（local error)
所以 react的core thought -- thoughts + actions 后跟observation环境反馈+rewards 让思考和actions形成正反馈，推理帮助规划纠错跟踪进度， actions从外部补充事实，降低幻觉
结论-- react better zhan only acting or only inference

文章里主要用到了四个数据集或者说是四种应用场景-- intensive-knowledge task ：多跳问答，事实验证（fever）； 决策任务：文字游戏，网页导航

introduction -- 人为什么这么做， 模型也应该这么做
人类会在动作之间用语言化的独白去计划，更新，查缺补漏； react模拟这种行为； 
only inference 会传播错误和幻觉； only action会缺少高层的规划； 
adopt thought - action - observe -- make task procedure stable

这里用了两个任务类型 -- 问答，事实核验任务 / 交互式决策


methodology -- 
交互式智能体的设定 -- 
time step t: 环境给观测o_t,智能体按照策略𝜋(a_t | c_t) 选取动作a_t，其中上下文c_t = ()
总的来说这个公式其实就是在给我们定义context到底包含了什么，以及react为什么要把过去做过的事情也放进去

o_k 是第k步obs，比如是维基百科返回的一段文字
a_k 是第k步action，比如是search[einstein].lookup['nobel']

也就是说 到第t步为止， agent能看到的一切的可用信息，就是从一开始到现在按照时间顺序交替发生的观察和动作，再加上当前的观察o_t，这一整段序列就是当前的上下文c_t
这个上线的长度和模型本身的能力有关，所以在代码里我设置了这个循环step为3，原文是8；

如果你只看o_t 不看前面的claim，obs，action，那么model就很容易误解这个obs的语境，举个例子，就是model并不知道这个查找的页面从哪里来，是否checkout过页面，这个人称代替到底是谁等
如果把action也放进上下文 那么就等同于把tracing全部保留 有利于reasoning的时候回看信息细节，并且决定下一步应该做什么，比如上一步还是lookup 下一步就应该是search 而不是其他动作

从决策建模的角度来说， 这是一种部分可观测的做法，因为单次观测o_t不能完整的刻画世界状态，所以把这个那个看历史当成信息充分的状态来喂给当前策略pai

然后react会吧动作空间扩展为 A_ = A 交集 L_1
其中A是真实的环境动作 search lookup finish
L_1是语言动作，也就是thought，不改变外部环境 但会被下入上下文 让后续决策可用

于是 我们的更新策略就被分为两种 
决策主导，比如执行环境动作a_t 属于环境A，那么就有c_t+1 = (c_t, a_t, o_t+1),你做了事情环境返回观察然后把这俩都接在来历史后面
稀疏地插入 Thought（只在关键节点想一下），其他大多是连续的环境动作。
目的：维持节奏与效率，避免“每步都碎碎念”拖慢和扰乱行动。

如果执行语言动作～a_t 属于L：
那么就有c_t+1 = (c_t, ~a_t)， 可以类比于你思考了一次 然后把这思考接到历史后面， 但是没新的观察因为这里不触发环境改变；
这种策略的关键在于抽丝剥茧的multi hop推理与证据核对，密集的交替thought - action - observation 基本每一步都有thought
目的是为了降低幻觉和错链


在这里作者提到语言空间是无限的，因为是自由文本所以趋近无限，对于策略学习来说，在这样的一个巨大动作空间里学习几乎是不可实现的。
这里我认为是因为policy会非常难以探索，无穷多表达方式都能代表近似的想法；这会让model非常难以收敛，同一个intent，千万种不同的措辞，credit assifnment和stability都会非常的差
这就需要强语言先验，即pretraining过的llm，以及学会了怎么合理的表述自然语言，还有如何把语言与世界知识对齐，也就是model内部已经有合理的知识概率分布。靠着这种先验，
我们就不用从0开始学习而是提示model 怎么来写对话式动作，（补充一点，如果能用instruct就别用base model，尤其是如果你的任务result比较solid，比如你要输出结构性数据）

所以作者在这直接用的冻结palm-540b的模型做few shot prompting，不做rl/sft；
每条example都是人类标注过的精数据，完整的tracing，附录里有样例，我们可以稍厚直接看代码
这样模型就能够同时生成damain-specific actions -- search lookup finish
分解claim，记下key point，plan next step，判断是否结束

这相当于把学习策略的难题转化为写好提示+让llm按照格式模仿；但这个我觉得还是有待商榷这个方法的实用性

react四个特性：
直观，易设计：数据标注不用特殊模版和技巧
通用灵活：thought的自由度+thought/chain的弹性出现，使得react跨任务适用；   动作集合不同推理需求不同但react都可以handle
性能强鲁棒性高：只靠1-6条few shot 就可以在多个领域显著超过只推理和只行动的baseline
论文还在附录 A.1 报了GPT-3 比 PaLM-540B 更好的结果（提示：选择更强先验的 LLM，会更强）。这儿可以讨论
可控且可解释；这个是我觉得最重要的 因为可以完整tracing，人为可以进行干预，人在运行时可以编辑 Thought（Figure 5 示范），相当于给 agent“改计划/纠错”，提升可控性与安全性。

这里我觉得可以拿出来讨论的点：

为何“强语言先验”是关键？
因为我们不是训练一个策略去在无限语言空间里“学会写”，而是让预训练巨模用它已有的语言与世界知识能力，直接按例子模仿“如何交替 思考–行动–观察”。这本质上是用生成式建模替代策略搜索。

为何“密集 vs 稀疏 Thought”要区分？
密集能减少幻觉、让证据链可核验；但在长行动序列里太密会导致冗余、节奏慢、甚至分散注意力。把“何时想”交还给模型（或在 prompt 里示范“关键点才想”）能更高效。

“无 ad-hoc 格式/选择”真的不需要吗？
论文强调“简单示例即可”，但工程上你仍需要稳健的解析器（保证 Thought i: / Action i: 正确抽取）、大小写/空动作兜底，以及明确的动作语法（避免模型发散）。

人机协同的价值
Thought 轨迹能被在线改写，这是把 LLM 做成可操纵的策略而非“黑箱输出”的关键。对安全和可靠性尤其重要。

到这里基本react的base knowledge就结束了，剩下的两章就是两种不同任务类型的具体设置以及improvement


第三章是知识密集任务
这里主要是做了两个task hotpot qa 和 fever事实验证 都是要检索外部数据然后组织并核对数据做出结论

接口：对维基百科进行search动作，检索词条，然后lookup查找包含关键字的句子，最后finish输出答案并结束任务标记done。故意弱化工具破事模型显式写thought和evidance chain的弹性出现，使得react跨任务适用；

pipeline； 与wikipedia api交互， 模型reasoning决定接下来检索，然后用检索结果支撑reasoning

prompt engineering：
从训练集随机挑选数据样例，这里是选了六个和三个
few-shot 精标数据：每一个in-context 例子包含thought/action/observation交替到任务完成，对推理主导任务， 基本每步都有thought，显式的】
构建证据链，抑制幻觉
具体可以看fig1d，每条轨迹由多轮“思考—行动—观察”步骤组成（即密集思考，dense thought），其中自由形式的思考用于多种目的。
具体而言，我们结合了以下类型的思考：将问题分解（“我需要先搜索 x，再找到 y，然后找到 z”）、从维基百科的观察中抽取信息（“x 始于 1844 年”、“这段文字没有说明 x”）、
进行常识推理（“x 不是 y，因此 z 应该是……”）或算术推理（“1844 < 1989”）、引导搜索重述（“也许我可以改为搜索/查找 x”），以及综合给出最终答案（“……所以答案是 x”）。

baseline fig1 a-c
(a) 标准提示（Standard）：移除 ReAct 轨迹中的所有“思考、行动、观察”。
(b) Chain-of-Thought 提示（CoT）（Wei 等，2022）：移除“行动”和“观察”，仅保留推理，作为纯推理基线。我们还构建了自一致性基线（CoT-SC）（Wang 等，2022a;b）：在推理时以温度 0.7 采样 21 条 CoT 轨迹，并采用多数答案；实践发现该方法能稳定提升相较于 CoT 的性能。
(c) 仅行动提示（Act）：移除 ReAct 轨迹中的“思考”，这在形式上与 WebGPT（Nakano 等，2021）通过互联网回答问题的交互方式有些相似，但其任务与动作空间不同，且使用的是模仿学习与强化学习，而非提示工程。


ReAct 所展现的解题过程更具事实性与落地性，而 CoT 在构建推理结构方面更准确，但容易出现事实或思维的幻觉。因此我们提出将 ReAct 与 CoT-SC 结合，并依据以下启发式让模型在两者之间切换：
A) ReAct → CoT-SC：当 ReAct 在给定步数内未能返回答案时，回退到 CoT-SC。我们将 HotpotQA 与 FEVER 的步数上限分别设为 7 和 5，因为进一步增加步数并不会提升 ReAct 的表现。
B) CoT-SC → ReAct：当 n 个 CoT-SC 样本中的多数答案出现次数少于 n/2（即内部知识对当前任务的支持可能不足）时，回退到 ReAct。
鉴于在大规模上手动标注推理轨迹与动作的难度，我们采用类似 Zelikman 等（2022）的自举方法：使用由 ReAct（以及其他基线）生成、且答案正确的 3000 条轨迹，对较小的语言模型（PaLM-8/62B）进行微调，使其在给定输入问题/主张时，能够解码出包含“思考、行动、观察”的完整轨迹。更多细节见附录 B.1。

官方结论：
ReAct 胜过 Act：有“思考→行动”的链路，能更好地把检索来的事实综合成最终答案。

ReAct vs CoT：在 FEVER（要判 SUPPORTS/REFUTES，细微事实差别很关键）里 ReAct 更好（60.9 vs 56.3）；在 HotpotQA 里 CoT 略好（29.4 vs 27.4）。

组合更强：ReAct ↔ CoT-SC 的双向回退最强：只用 3–5 个样本就能达到 CoT-SC(21) 的表现。

微调更显著：用 3,000 条 ReAct 轨迹微调后，小模型 ReAct-FT > 大模型提示（8B-FT>62B-prompt；62B-FT>540B-prompt）。

为什么：从成功/失败模式看差异（表 2 的核心）

成功（正确）里：

CoT 幻觉更多：成功样本里 CoT 的“假阳性”（幻觉的推理或事实）= 14%，ReAct=6%。
→ 原因：CoT 只靠“内知识”，容易“想当然”；ReAct 有外部检索，更落地。

失败里：

ReAct 的主要失败=推理错误（47%）：结构化的“思考-行动-观察”让它更踏实，但灵活性差些，容易在重复思考/动作里打转（未能决定“下一步该做什么”）。

ReAct 对检索质量很敏感：23% 的错误来自“search 结果无信息或含金量低”，把后续推理带偏。

CoT 的主要失败=幻觉（56%）：没有外部证据束缚，纯内生的错误占大头。

标签含糊两者接近（~28–29%）：有些题目标注本身就含糊或过时。

具体数字与设定（和你代码密切相关）

步数上限：他们用 HotpotQA=7、FEVER=5；继续加步数并不会提升 ReAct 表现。

CoT-SC：n=21、温度 0.7，多数票 > n/2（≥11） 才算自信。

样本效率：ReAct + CoT-SC 只需 3–5 个样本就能追平 CoT-SC(21) 的效果。

微调：用 3k 条“含思考+动作+观察”的正确轨迹微调后，

PaLM-8B（ReAct-FT） > 所有 PaLM-62B 的提示版；

PaLM-62B（ReAct-FT） > 所有 540B 的提示版；

说明：学会“怎么检索+怎么用检索”比背知识更可迁移。微调 Standard/CoT 明显不如 ReAct/Act

第四章交互式决策
关键结果
ALFWorld（表3）

ReAct（best of 6）：71% 平均成功率

Act（best of 6）：45%

BUTLER（best of 8）：37%

甚至 ReAct（worst of 6）= 48% 也超过 Act/BUTLER 的最优试验。

ReAct 相对 Act 的提升在 6 次对照里一致，相对增益 33%–90%，平均 62%。

失败对比：Act 常见不会合理分解目标或跟踪当前状态；ReAct 用稀疏 Thought 在关键节点“定方向/记进度/换策略”。

WebShop（表4）

Act：Score 62.3 / SR 30.1（一小样本提示就能接近 IL/IL+RL）

ReAct：Score 66.6 / SR 40.0（+10% 绝对提升，显著优于 IL 与 IL+RL）

Human：Score 82.1 / SR 59.6（与人类还有差距——人类会做更多探索与检索重构，当前纯提示方法仍欠缺

key point: sparse though 的构建：
[需要思考(THINK)的可检测时机 - Plain Text]

1) 重复/没进展
   - 连续 k 步 Observation 基本一致，或包含 "No more results" / "No result" 等提示
   - 连续 k 步访问/点击同一类别页面（商品/房间未变化）
   => 触发：生成一行 Thought，总结现状 + 提出新的搜索/导航策略

2) 分叉变多（分支爆炸）
   - 当前可选动作数量激增（> branch_threshold），如 WebShop 出现大量相似候选项
   => 触发：生成一行 Thought，给出筛选标准/排序依据（先过滤哪个条件）

3) 约束冲突 / 信息矛盾
   - 预算超限、颜色/规格不匹配
   - Observation 与先前假设/计划矛盾
   => 触发：生成一行 Thought，定位冲突 + 重写约束/计划

4) 子目标完成 / 切换节点
   - 任务清单中的一个子目标已完成，需要决定下一个子目标
   => 触发：生成一行 Thought，明确下一步计划/子目标

5) 不确定性高
   - 语言模型生成的动作 top-1 与 top-2 概率接近 (|p1 - p2| < epsilon)
   - 或整体 logprob 很低/模型犹豫
   => 触发：生成一行 Thought，先思考后再选择 Action

6) 查询失败
   - search / lookup 连续失败 k 次
   => 触发：生成一行 Thought，改写查询或更换实体/路径

直觉总结：
- 没进展、分叉多、起冲突、要切换、很犹豫、常失败 —— 就该“想一下”（输出一行 Thought）。

最后的相关工作：
用于推理的语言模型（LM for reasoning）

代表：Chain-of-Thought (CoT) 揭示了大模型可用“逐步思考”解题；随后有 Least-to-most、Zero-shot CoT、Self-consistency 等扩展；也有把推理流程拆分或自举的框架（Selection–Inference、STaR、Scratchpad、Faithful reasoning 等）。这些方法多聚焦于固定/隔离的推理过程，并不与外部交互紧密结合。

ReAct

ReAct 的不同：不仅做“静态推理”，而是把动作及其观察串入同一输入流，让模型在“推理—行动—观察”的闭环中更准确地推理，并能处理超出纯推理的问题（如交互式决策）。

ReAct

用于决策的语言模型（LM for decision making）

代表：WebGPT 等利用 LM 驱动网页交互、再配合大量人类反馈做强化学习；对话/任务型系统（BlenderBot、Sparrow、SimpleTOD）也让 LM 决策 API 调用，但依赖昂贵数据与人类标注；在具身/交互环境里，SayCan、Inner Monologue (IM) 用 LM 做动作规划或把环境反馈注入“内心独白”。

ReAct

ReAct 的不同：不依赖大规模 RLHF 或昂贵标注，而是用少量 in-context 轨迹就能把“思考（Thought）”与“行动（Action）”整合到统一轨迹里，形成可解释且成本更低的策略学习方式。

