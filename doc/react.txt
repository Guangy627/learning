问题-- llm在reasoning和acting上各有进展但是2023年以前两者都是分开的，reasoning cot 让模型生成一大串思维步骤，但是这些推理是封闭的，完全依赖的是模型内部的知识结构
容易油hallucination； acting model 基于语言的计划与execute比如说控制agnet在环境中的routing，但是这种方法本身会缺乏系统性的推理容易陷入错误（local error)
所以 react的core thought -- thoughts + actions 后跟observation环境反馈+rewards 让思考和actions形成正反馈，推理帮助规划纠错跟踪进度， actions从外部补充事实，降低幻觉
结论-- react better zhan only acting or only inference

文章里主要用到了四个数据集或者说是四种应用场景-- intensive-knowledge task ：多跳问答，事实验证（fever）； 决策任务：文字游戏，网页导航

introduction -- 人为什么这么做， 模型也应该这么做
人类会在动作之间用语言化的独白去计划，更新，查缺补漏； react模拟这种行为； 
only inference 会传播错误和幻觉； only action会缺少高层的规划； 
adopt thought - action - observe -- make task procedure stable

这里用了两个任务类型 -- 问答，事实核验任务 / 交互式决策


methodology -- 
交互式智能体的设定 -- 
time step t: 环境给观测o_t,智能体按照策略𝜋(a_t | c_t) 选取动作a_t，其中上下文c_t = ()
总的来说这个公式其实就是在给我们定义context到底包含了什么，以及react为什么要把过去做过的事情也放进去

o_k 是第k步obs，比如是维基百科返回的一段文字
a_k 是第k步action，比如是search[einstein].lookup['nobel']

也就是说 到第t步为止， agent能看到的一切的可用信息，就是从一开始到现在按照时间顺序交替发生的观察和动作，再加上当前的观察o_t，这一整段序列就是当前的上下文c_t
这个上线的长度和模型本身的能力有关，所以在代码里我设置了这个循环step为3，原文是8；

如果你只看o_t 不看前面的claim，obs，action，那么model就很容易误解这个obs的语境，举个例子，就是model并不知道这个查找的页面从哪里来，是否checkout过页面，这个人称代替到底是谁等
如果把action也放进上下文 那么就等同于把tracing全部保留 有利于reasoning的时候回看信息细节，并且决定下一步应该做什么，比如上一步还是lookup 下一步就应该是search 而不是其他动作

从决策建模的角度来说， 这是一种部分可观测的做法，因为单次观测o_t不能完整的刻画世界状态，所以把这个那个看历史当成信息充分的状态来喂给当前策略pai

然后react会吧动作空间扩展为 A_ = A 交集 L_1
其中A是真实的环境动作 search lookup finish
L_1是语言动作，也就是thought，不改变外部环境 但会被下入上下文 让后续决策可用

于是 我们的更新策略就被分为两种 
决策主导，比如执行环境动作a_t 属于环境A，那么就有c_t+1 = (c_t, a_t, o_t+1),你做了事情环境返回观察然后把这俩都接在来历史后面
稀疏地插入 Thought（只在关键节点想一下），其他大多是连续的环境动作。
目的：维持节奏与效率，避免“每步都碎碎念”拖慢和扰乱行动。

如果执行语言动作～a_t 属于L：
那么就有c_t+1 = (c_t, ~a_t)， 可以类比于你思考了一次 然后把这思考接到历史后面， 但是没新的观察因为这里不触发环境改变；
这种策略的关键在于抽丝剥茧的multi hop推理与证据核对，密集的交替thought - action - observation 基本每一步都有thought
目的是为了降低幻觉和错链


在这里作者提到语言空间是无限的，因为是自由文本所以趋近无限，对于策略学习来说，在这样的一个巨大动作空间里学习几乎是不可实现的。
这里我认为是因为policy会非常难以探索，无穷多表达方式都能代表近似的想法；这会让model非常难以收敛，同一个intent，千万种不同的措辞，credit assifnment和stability都会非常的差
这就需要强语言先验，即pretraining过的llm，以及学会了怎么合理的表述自然语言，还有如何把语言与世界知识对齐，也就是model内部已经有合理的知识概率分布。靠着这种先验，
我们就不用从0开始学习而是提示model 怎么来写对话式动作，（补充一点，如果能用instruct就别用base model，尤其是如果你的任务result比较solid，比如你要输出结构性数据）

所以作者在这直接用的冻结palm-540b的模型做few shot prompting，不做rl/sft；
每条example都是人类标注过的精数据，完整的tracing，附录里有样例，我们可以稍厚直接看代码
这样模型就能够同时生成damain-specific actions -- search lookup finish
分解claim，记下key point，plan next step，判断是否结束

这相当于把学习策略的难题转化为写好提示+让llm按照格式模仿；但这个我觉得还是有待商榷这个方法的实用性

react四个特性：
直观，易设计：数据标注不用特殊模版和技巧
通用灵活：thought的自由度+thought/chain的弹性出现，使得react跨任务适用；   动作集合不同推理需求不同但react都可以handle
性能强鲁棒性高：只靠1-6条few shot 就可以在多个领域显著超过只推理和只行动的baseline
论文还在附录 A.1 报了GPT-3 比 PaLM-540B 更好的结果（提示：选择更强先验的 LLM，会更强）。这儿可以讨论
可控且可解释；这个是我觉得最重要的 因为可以完整tracing，人为可以进行干预，人在运行时可以编辑 Thought（Figure 5 示范），相当于给 agent“改计划/纠错”，提升可控性与安全性。

这里我觉得可以拿出来讨论的点：

为何“强语言先验”是关键？
因为我们不是训练一个策略去在无限语言空间里“学会写”，而是让预训练巨模用它已有的语言与世界知识能力，直接按例子模仿“如何交替 思考–行动–观察”。这本质上是用生成式建模替代策略搜索。

为何“密集 vs 稀疏 Thought”要区分？
密集能减少幻觉、让证据链可核验；但在长行动序列里太密会导致冗余、节奏慢、甚至分散注意力。把“何时想”交还给模型（或在 prompt 里示范“关键点才想”）能更高效。

“无 ad-hoc 格式/选择”真的不需要吗？
论文强调“简单示例即可”，但工程上你仍需要稳健的解析器（保证 Thought i: / Action i: 正确抽取）、大小写/空动作兜底，以及明确的动作语法（避免模型发散）。

人机协同的价值
Thought 轨迹能被在线改写，这是把 LLM 做成可操纵的策略而非“黑箱输出”的关键。对安全和可靠性尤其重要。

到这里基本react的base knowledge就结束了，剩下的两章就是两种不同任务类型的具体设置以及improvement


第三章是知识密集任务
这里主要是做了两个task hotpot qa 和 fever事实验证 都是要检索外部数据然后组织并核对数据做出结论

接口：对维基百科进行search动作，检索词条，然后lookup查找包含关键字的句子，最后finish输出答案并结束任务标记done。故意弱化工具破事模型显式写thought和evidance chain的弹性出现，使得react跨任务适用；

pipeline； 与wikipedia api交互， 模型reasoning决定接下来检索，然后用检索结果支撑reasoning

prompt engineering：
从训练集随机挑选数据样例，这里是选了六个和三个
few-shot 精标数据：每一个in-context 例子包含thought/action/observation交替到任务完成，对推理主导任务， 基本每步都有thought，显式的】
构建证据链，抑制幻觉
具体可以看fig1d，每条轨迹由多轮“思考—行动—观察”步骤组成（即密集思考，dense thought），其中自由形式的思考用于多种目的。
具体而言，我们结合了以下类型的思考：将问题分解（“我需要先搜索 x，再找到 y，然后找到 z”）、从维基百科的观察中抽取信息（“x 始于 1844 年”、“这段文字没有说明 x”）、
进行常识推理（“x 不是 y，因此 z 应该是……”）或算术推理（“1844 < 1989”）、引导搜索重述（“也许我可以改为搜索/查找 x”），以及综合给出最终答案（“……所以答案是 x”）。

baseline fig1 a-c
(a) 标准提示（Standard）：移除 ReAct 轨迹中的所有“思考、行动、观察”。
(b) Chain-of-Thought 提示（CoT）（Wei 等，2022）：移除“行动”和“观察”，仅保留推理，作为纯推理基线。我们还构建了自一致性基线（CoT-SC）（Wang 等，2022a;b）：在推理时以温度 0.7 采样 21 条 CoT 轨迹，并采用多数答案；实践发现该方法能稳定提升相较于 CoT 的性能。
(c) 仅行动提示（Act）：移除 ReAct 轨迹中的“思考”，这在形式上与 WebGPT（Nakano 等，2021）通过互联网回答问题的交互方式有些相似，但其任务与动作空间不同，且使用的是模仿学习与强化学习，而非提示工程。


ReAct 所展现的解题过程更具事实性与落地性，而 CoT 在构建推理结构方面更准确，但容易出现事实或思维的幻觉。因此我们提出将 ReAct 与 CoT-SC 结合，并依据以下启发式让模型在两者之间切换：
A) ReAct → CoT-SC：当 ReAct 在给定步数内未能返回答案时，回退到 CoT-SC。我们将 HotpotQA 与 FEVER 的步数上限分别设为 7 和 5，因为进一步增加步数并不会提升 ReAct 的表现。
B) CoT-SC → ReAct：当 n 个 CoT-SC 样本中的多数答案出现次数少于 n/2（即内部知识对当前任务的支持可能不足）时，回退到 ReAct。
鉴于在大规模上手动标注推理轨迹与动作的难度，我们采用类似 Zelikman 等（2022）的自举方法：使用由 ReAct（以及其他基线）生成、且答案正确的 3000 条轨迹，对较小的语言模型（PaLM-8/62B）进行微调，使其在给定输入问题/主张时，能够解码出包含“思考、行动、观察”的完整轨迹。更多细节见附录 B.1。

